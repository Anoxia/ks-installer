apiVersion: v1
items:
- apiVersion: alerting.kubesphere.io/v2beta1
  kind: GlobalRuleGroup
  metadata:
    annotations:
      alerting.kubesphere.io/initial-configuration: '{"apiVersion":"alerting.kubesphere.io/v2beta1","kind":"GlobalRuleGroup","metadata":{"annotations":{},"labels":{"alerting.kubesphere.io/builtin":"true","alerting.kubesphere.io/enable":"true"},"name":"general-rules","namespace":"kubesphere-monitoring-system"},"spec":{"rules":[{"alert":"TargetDown","annotations":{"description":"{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.","runbook_url":"https://alert-runbooks.kubesphere.io/runbooks/general/targetdown","summary":"One or more targets are unreachable."},"expr":"100 * (count(up == 0) BY (job, namespace, service, cluster) / count(up) BY (job, namespace, service, cluster)) > 10\n","for":"10m","labels":{"rule_id":"92cc619e8c5062c3d16f97b5f7cacee1"},"severity":"warning"},{"alert":"Watchdog","annotations":{"description":"This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\n","runbook_url":"https://alert-runbooks.kubesphere.io/runbooks/general/watchdog","summary":"An alert that should always be firing to certify that Alertmanager is working properly."},"expr":"vector(1)","labels":{"rule_id":"c3a7cdf06aa9577e3fd8c11337e4c002"},"severity":"none"},{"alert":"InfoInhibitor","annotations":{"description":"This is an alert that is used to inhibit info alerts.\nBy themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with\nother alerts.\nThis alert fires whenever there''s a severity=\"info\" alert, and stops firing when another alert with a\nseverity of ''warning'', ''error'' or ''critical'' starts firing on the same namespace.\nThis alert should be routed to a null receiver and configured to inhibit alerts with severity=\"info\".\n","runbook_url":"https://alert-runbooks.kubesphere.io/runbooks/general/infoinhibitor","summary":"Info-level alert inhibition."},"expr":"ALERTS{severity = \"info\"} == 1 unless on(namespace, cluster) ALERTS{alertname != \"InfoInhibitor\", severity =~ \"warning|error|critical\", alertstate=\"firing\"} == 1\n","labels":{"rule_id":"b9d0e3139b46cab8ff57b28b1e1f127b"},"severity":"none"}]}}'
    labels:
      alerting.kubesphere.io/builtin: "true"
      alerting.kubesphere.io/enable: "true"
    name: general-rules
    namespace: kubesphere-monitoring-system
  spec:
    rules:
    - alert: TargetDown
      annotations:
        description: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.'
        runbook_url: https://alert-runbooks.kubesphere.io/runbooks/general/targetdown
        summary: One or more targets are unreachable.
      expr: |
        100 * (count(up == 0) BY (job, namespace, service, cluster) / count(up) BY (job, namespace, service, cluster)) > 10
      for: 10m
      labels:
        rule_id: 92cc619e8c5062c3d16f97b5f7cacee1
      severity: warning
    - alert: Watchdog
      annotations:
        description: |
          This is an alert meant to ensure that the entire alerting pipeline is functional.
          This alert is always firing, therefore it should always be firing in Alertmanager
          and always fire against a receiver. There are integrations with various notification
          mechanisms that send a notification when this alert is not firing. For example the
          "DeadMansSnitch" integration in PagerDuty.
        runbook_url: https://alert-runbooks.kubesphere.io/runbooks/general/watchdog
        summary: An alert that should always be firing to certify that Alertmanager is working properly.
      expr: vector(1)
      labels:
        rule_id: c3a7cdf06aa9577e3fd8c11337e4c002
      severity: none
    - alert: InfoInhibitor
      annotations:
        description: |
          This is an alert that is used to inhibit info alerts.
          By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with
          other alerts.
          This alert fires whenever there's a severity="info" alert, and stops firing when another alert with a
          severity of 'warning', 'error' or 'critical' starts firing on the same namespace.
          This alert should be routed to a null receiver and configured to inhibit alerts with severity="info".
        runbook_url: https://alert-runbooks.kubesphere.io/runbooks/general/infoinhibitor
        summary: Info-level alert inhibition.
      expr: |
        ALERTS{severity = "info"} == 1 unless on(namespace, cluster) ALERTS{alertname != "InfoInhibitor", severity =~ "warning|error|critical", alertstate="firing"} == 1
      labels:
        rule_id: b9d0e3139b46cab8ff57b28b1e1f127b
      severity: none
- apiVersion: alerting.kubesphere.io/v2beta1
  kind: GlobalRuleGroup
  metadata:
    annotations:
      alerting.kubesphere.io/initial-configuration: '{"apiVersion":"alerting.kubesphere.io/v2beta1","kind":"GlobalRuleGroup","metadata":{"annotations":{},"labels":{"alerting.kubesphere.io/builtin":"true","alerting.kubesphere.io/enable":"true"},"name":"node-network","namespace":"kubesphere-monitoring-system"},"spec":{"rules":[{"alert":"NodeNetworkInterfaceFlapping","annotations":{"description":"Network interface \"{{ $labels.device }}\" changing its up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}","runbook_url":"https://alert-runbooks.kubesphere.io/runbooks/general/nodenetworkinterfaceflapping","summary":"Network interface is often changing its status"},"expr":"changes(node_network_up{job=\"node-exporter\",device!~\"veth.+\"}[2m]) > 2\n","for":"2m","labels":{"rule_id":"62864be7b7ed1b8a84939fefadca1167"},"severity":"warning"}]}}'
    labels:
      alerting.kubesphere.io/builtin: "true"
      alerting.kubesphere.io/enable: "true"
    name: node-network
    namespace: kubesphere-monitoring-system
  spec:
    rules:
    - alert: NodeNetworkInterfaceFlapping
      annotations:
        description: Network interface "{{ $labels.device }}" changing its up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}
        runbook_url: https://alert-runbooks.kubesphere.io/runbooks/general/nodenetworkinterfaceflapping
        summary: Network interface is often changing its status
      expr: |
        changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) > 2
      for: 2m
      labels:
        rule_id: 62864be7b7ed1b8a84939fefadca1167
      severity: warning
kind: List
